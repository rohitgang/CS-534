{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "north-supply",
   "metadata": {},
   "source": [
    "# PS2\n",
    "## Rohit Gangurde\n",
    "## Collaborators : None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-banana",
   "metadata": {},
   "source": [
    "### Probability of predictions :\n",
    "\n",
    "> $$ h_{\\theta}(x) = \\frac{1}{1 + e^{-(w.x+b)}} $$\n",
    "    \n",
    "> where, $ \\theta$ $=$ $ [b, w_{1}, w_{2}, ... , w_{d}] $\n",
    "\n",
    "### Sigmoid/Logistic Function :\n",
    "\n",
    "> $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "### Maximum Likelihood Estimator\n",
    "\n",
    "> $$ L(\\theta) = P(y|X;\\theta) = \\prod_{i=1}^{m} P(y^{i}|x^{i};\\theta) $$\n",
    ">\n",
    "> When, $ y = 1$, $ P(y|x;\\theta)$ is $ h_{\\theta}(x)$\n",
    ">\n",
    "> When, $ y = 0$, $ P(y|x;\\theta)$ is $ 1 - h_{\\theta}(x)$\n",
    ">\n",
    "> Therefore, $ P(y|x;\\theta) = h_{\\theta}(x)^{y} . (1 - h_{\\theta}(x))^{1-y} $\n",
    ">\n",
    "> The MLE function now becomes,\n",
    ">\n",
    ">> $$ L(\\theta) = \\prod_{i=1}^{m} P(y^{i}|x^{i};\\theta) = \\prod_{i=1}^{m} h_{\\theta}(x^{i})^{y^{i}} . (1 - h_{\\theta}(x^{i}))^{1-y^{i}}  $$\n",
    ">\n",
    "> We take the log of the this function to simplify calculations and avoid underflow when multiplying small numbers.\n",
    ">\n",
    "> Hence, the log likelihood function is,\n",
    ">\n",
    ">> $$ log L(\\theta) = \\sum_{i=1}^{m} y^{i} . log[h_{\\theta}(x^{i})] + (1 - y^{i}) . log(1 - h_{\\theta}(x^{i})) $$\n",
    "\n",
    "### Gradient Ascent\n",
    "\n",
    "> Applying gradient ascent to log likelihood function,\n",
    ">\n",
    ">> $$ \\nabla_{\\theta} log L(\\theta) = \\nabla_{\\theta} \\sum_{i=1}^{m} y^{i} . log[h_{\\theta}(x^{i})] + (1 - y^{i}) . log(1 - h_{\\theta}(x^{i})) $$\n",
    ">>\n",
    ">>    $$ = \\sum_{i=1}^{m} \\frac{y^{i}}{h_{\\theta}(x)} \\nabla_{\\theta} h_{\\theta} (x^{i}) + \\frac{1 - y^{i}}{1 - h_{\\theta}(x^{i})} (-\\nabla_{\\theta} h_{\\theta} (x^{i})) $$ \n",
    ">\n",
    "> After evaluating $ \\nabla_{\\theta} h_{\\theta}(x) $, \n",
    ">\n",
    ">> $$ \\nabla_{\\theta} log L(\\theta) = \\sum_{i=1}^{m} x^{i} . (y^{i} - h_{\\theta}(x^{i})) $$\n",
    ">\n",
    "> For faster implementation, we vectorize our calculations. \n",
    ">\n",
    ">> $ w^{t} \\rightarrow w^{t-1} + \\eta \\sum_{i=1}^{m} x^{i} . (y^{i} - h_{\\theta(t-1)}(x^{i})) $ for all $j$ $\\geq$ $1$\n",
    ">>\n",
    "> >$ b^{t} \\rightarrow b^{t - 1} + \\eta \\sum_{i=1}^{m} (y^{i} - h_{\\theta(t-1)}(x^{i})) $\n",
    "\n",
    "### Penalizing with L2 regularization\n",
    "\n",
    "> We add L2 regularization to avoid overfitting.\n",
    ">\n",
    ">> $$ Objective(\\theta) = log L(\\theta) - \\frac{\\alpha}{2} ||\\theta||^{2} $$\n",
    ">>\n",
    ">>   $$ = log L(\\theta) - \\frac{\\alpha}{2} \\sum_{j=1}^{d} \\theta_{j}^{2} $$ where $\\alpha$ is the regularization weight.\n",
    ">\n",
    "> Hence, this changes the updates made to $w$,\n",
    ">\n",
    ">> $$ w \\rightarrow w + \\sum_{i=1}^{m} (x^{i} . (y^{i} - h_{\\theta} (x^{i}))) - 2\\alpha w $$\n",
    ">\n",
    "> We don't penalize $b$ because it doesn't grow that large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "occasional-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class l2_logistic_regression():\n",
    "    \"\"\"\n",
    "        A logistic regression class implemented with l2 regularization and gradient ascent. The attributes of the class are :\n",
    "            weights : the weight coefficients for features of the dataset\n",
    "            logs : list of log likelihood estimates\n",
    "            epochs : determines max iterations of the function\n",
    "            learning_rate : determines by how much to step our values\n",
    "            lamda_ : the regularization strength parameter for l2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.logs = None\n",
    "        self.epochs = None\n",
    "        self.learning_rate = None\n",
    "        self.lambda_ = None\n",
    "\n",
    "    def h_theta(self, x, w) :\n",
    "        \"\"\"\n",
    "            Returns the probability of the weighted sum.\n",
    "            Parameters :\n",
    "                x : dataset features\n",
    "                w : weight coefficients\n",
    "        \"\"\"\n",
    "        w_x = np.dot(x, w)\n",
    "        return (1 / (1 + np.exp(-w_x)))\n",
    "\n",
    "    def log_likelihood(self, x, y, w):\n",
    "        \"\"\"\n",
    "            Computes the log likelihood estimates.\n",
    "            Parameters : \n",
    "                x : dataset features\n",
    "                w : weight coefficients\n",
    "                y : the target class\n",
    "        \"\"\"\n",
    "        \n",
    "        ans = 0\n",
    "        for i in range(len(x)):\n",
    "            preds = np.log(self.h_theta(x[i], w))\n",
    "            inv_preds = np.log(1 - self.h_theta(x[i], w))\n",
    "            inv_y = 1 - y[i]\n",
    "            ans += y[i]*preds + inv_y*inv_preds\n",
    "        return ans\n",
    "\n",
    "    def log_derivative(self, x, errors, w, is_bias):\n",
    "        \"\"\"\n",
    "            Computes the derivative of the l2 log likelihood function to make updates \n",
    "            to the weight coefficients\n",
    "            Parameters :\n",
    "                x : dataset features\n",
    "                w : weight coefficients\n",
    "                errors : error between predictions and true values\n",
    "                is_bias : updates to weights use the L2 regularizstion parameter. For the \n",
    "                          bias coefficient, we don't use L2. So, this is a indicator for \n",
    "                          the bias coefficient.\n",
    "        \"\"\"\n",
    "        \n",
    "        derivative = np.dot(np.transpose(errors), x)\n",
    "        if not is_bias :\n",
    "            derivative -= 2*self.lambda_ * w\n",
    "        return derivative\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, lambda_=0.001, lr=1e-7):\n",
    "        \"\"\"\n",
    "            Implements the fit procedure for the logistic regression model with \n",
    "            l2 regularization and gradient ascent.\n",
    "            Parameters :\n",
    "                X : features of the dataset\n",
    "                y : target value of the dataset\n",
    "                epochs : number of iterations\n",
    "                lambda_ : the penalty parameter, also lambda_ = 1/c\n",
    "                lr : learning rate for our function\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = lr\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "        b = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((b, X))\n",
    "        w = np.zeros((X.shape[1], 1))\n",
    "        ll = list()\n",
    "        for i in range(self.epochs) :\n",
    "            predictions = self.h_theta(X, w)\n",
    "            errors = np.transpose(np.array([y])) - predictions\n",
    "            for j in range(len(w)):\n",
    "                at_bias = (j==0)\n",
    "                derivative = self.log_derivative(X[:,j], errors, w[j], at_bias)\n",
    "                w[j] += self.learning_rate * derivative\n",
    "            ll.append(self.log_likelihood(X, y, w))\n",
    "        \n",
    "        self.logs = ll\n",
    "        self.weights = w\n",
    "\n",
    "    def predict(self, x_vals) :\n",
    "        \"\"\"\n",
    "            Predict the target label\n",
    "        \"\"\"\n",
    "        x_vals = np.array(x_vals)\n",
    "        bias = np.ones((x_vals.shape[0], 1))\n",
    "        x_vals = np.hstack((bias, x_vals))\n",
    "        preds = list()\n",
    "        for x in x_vals:\n",
    "            y_hat = self.h_theta(x, self.weights)\n",
    "            preds.append(y_hat)\n",
    "        # Convert from probabilities to target label\n",
    "        preds = np.asarray(preds).flatten() >= 0.5\n",
    "        return preds\n",
    "    \n",
    "    def plot_likelihood(self):\n",
    "        \"\"\"\n",
    "            Plot the likelihood estimates\n",
    "        \"\"\"\n",
    "        x = np.linspace(0, len(self.logs), len(self.logs))\n",
    "        fig = plt.figure()\n",
    "        plt.plot(x, self.logs)\n",
    "        fig.suptitle('Training classifier with L2')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Log Likelihood')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-present",
   "metadata": {},
   "source": [
    "## Estimation of paramter 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desperate-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features, target = load_iris(return_X_y=True)\n",
    "\"\"\"\n",
    "    Target classes : Iris Setosa = 0\n",
    "                     Iris Versicolour = 1\n",
    "                     Iris Virginica = 2\n",
    "    Binary target value : Iris Setosa = 0\n",
    "                          Others = 1\n",
    "\"\"\"\n",
    "target = [1 if y == 0 else 0 for y in target]\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pretty-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C : 0.001, f1 : 0.18000000000000005\n",
      "C : 0.005, f1 : 0.6900000000000001\n",
      "C : 0.01, f1 : 0.7\n",
      "C : 0.05, f1 : 1.0\n",
      "C : 0.1, f1 : 1.0\n",
      "C : 0.5, f1 : 1.0\n",
      "C : 1, f1 : 1.0\n",
      "C : 5, f1 : 1.0\n",
      "C : 10, f1 : 1.0\n",
      "C : 50, f1 : 1.0\n",
      "C : 100, f1 : 1.0\n",
      "C : 500, f1 : 1.0\n",
      "C : 1000, f1 : 1.0\n",
      "C : 1000, f1 : 1.0\n",
      "C : 10000000, f1 : 1.0\n",
      "best C : 0.05\n",
      "best F1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "C = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 1000, 10000000]\n",
    "\n",
    "best_clf = None\n",
    "best_c = 0\n",
    "best_val = -1\n",
    "cv = 10\n",
    "y_train = np.array(y_train)\n",
    "for c in C :\n",
    "    avg_f1 = 0\n",
    "    k_folds = StratifiedKFold(n_splits=cv, random_state=10, shuffle=True)\n",
    "    clf = None\n",
    "    for train_idx, val_idx in k_folds.split(x_train, y_train):\n",
    "        x = x_train[train_idx]\n",
    "        x_val = x_train[val_idx]\n",
    "        y = y_train[train_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "        clf = l2_logistic_regression()\n",
    "        clf.fit(x, y, lambda_= 1/c, epochs=100, lr=0.001)\n",
    "        y_hat_val = clf.predict(x_val)\n",
    "        f1 = f1_score(y_val, y_hat_val, average='micro')\n",
    "        avg_f1 += f1\n",
    "        \n",
    "    avg_f1 = avg_f1 / cv\n",
    "    if best_val < avg_f1:\n",
    "        best_val = avg_f1\n",
    "        best_c = c\n",
    "        best_clf = clf\n",
    "    print('C : {}, f1 : {}'.format(c, avg_f1))\n",
    "        \n",
    "print('best C : {0}\\nbest F1 : {1}'.format(best_c, best_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patent-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = best_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bibliographic-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        31\n",
      "           1       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-absolute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.plot_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-boxing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A3",
   "language": "python",
   "name": "a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
